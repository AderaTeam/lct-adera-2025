# Исследование алгоритмов

В данном разделе представленны результаты исследования отзывов клиентов собранных с [banki.ru](https://www.banki.ru/)

# Описание данных

В результате работы парсера были собранны отзывы:
- **[banki.ru](https://www.banki.ru/)** 
    - число уникальных: 7000 шт.
    - данные представленны в `json` формате со следующими заголовками
        - *title* - заголовок отзыва;
        - *text* - текст отзыва;
        - *date_added* - дата создания отзыва; 

Все результаты эксперементов сохраняются в папку `research_data/experements_results/`
# Описание основных инструментов использованных в исследованиях

- [`Marimo`]() - динамически исполняемый ноутбук`Python` ноутбук
- [`KeyBERT`]() - для выделение ключевых слов в документе;
- [`BERTopic`]() - для тематическое моделирования;
- [`Spacy`]() - для генерации графа связности в предложении и выделения мусорных слов;


# Описание отчётов

В процесе анализа данных, были созданны отчёты, с использованием `marimo` - динамически исполняемых ноутбуков. 

В форме читаемого отчёта, ноутбуки храняться в папке `research_data/reports/`.

Исходный код ноутбуков представлен в папке `research_data/code/`, для запуска ноутбука, нужно выполнить следующий список команд:

- установка необходимых зависимостей, для большего удобства был использован пакетный мэнеджер `poetry`:

    ```bash
    poetry install
    ```

- запуск ноутбука (надо находиться в папке `research_data`):
    - для редактирования  (исполняемый код будет показан):
        ```
        poetry run marimo edit ./code/<имя_необходимого_ноутбука>
        ```
    - для просмотра (исполняемый код **не** будет показан)
        ```
        poetry run marimo run ./code/<имя_необходимого_ноутбука>
        ```

## Поиск топиков

Для поиска основных тем, существует несколько подходов:

- Кластеризация - стандартные алгоритмы кластеризации:
    - являются понятными/легко интерпритируемыми
    - позволяют легко оценивать полученный результат (локоть силуета)
    - но не позволяют легко оценивать поднятые топики внутри документа, т.е. документы предварительно надо ещё разделить 
- `LDA` - является стандартным подходом для тематического моделирования
    - слишком медленный и долгий
- `BERTopic` - современный подход заменивший LDA и объединяющий в себе ещё и кластеризацию


### Проверка метода KNN для выделенных топиков `./code/test_classification.py`

В данном ноутбуке проверяется гипотиза для классификации по топикам через поиск наиболее близкого по косинусному растоянию и подбираются модели 


### Тематическое моделирование данных `./code/topic_modelling.py`

В данном ноутбуке, помимо класического `BERTopic`, была рассмотрена его модификация с заменой `HDBSCAN` на `KMeans`

В добавок был рассмотрен алгоритм, который, помимо разбиения отзыва на подтемы, через представление представление его как графа леса и удаления слабых рёбер (рёбра писывающие отношение *`conj`*). При таком разбиении, себя лучше показывает модель `ai-forever/sbert_large_nlu_ru`

Для представления предложения в виде графа использовался [функционал](https://spacy.io/usage/visualizers) библиотеки `Spacy`


### Обучение сентементальной модели `train_sentiment_models.py`

При проверки семантических моделей, было выявленны следующие недостатки:

1. Большинство данных моделей имеют архитектуру `BERT`, что для такой простой задачи, не самое наилучшее решение, а так как в процессе работы алгоритма отзывы векторизуются - можно обучить `CatBoost` (как наиболее эффективную модель) исходя из логики - плохой *grade* - негативный отзыв


# Описание работы итогового алгоритма

- Дано:
    - Векторезатор: $V : Str \rightarrow \mathbb{R}^n$
    - Список топиков: $\mathbb{T} = \{x:Str\}$
    - Нижний предел для соответсвия подтексту топика: $\lambda \in \mathbb{R}$
    - Нижний предел для осмысленности топика: $k \in \mathbb{N}$

- Подготовительный этап: 
    1. Векторизуем топики

- Алгоритм
    1. На вход подаётся список отзывов $L_{reviews} = \{x:Str\}$
    2. Каждый из отзывов разбивается на подтекста (см. `../research`) $\overline{L}_{reviews} = \{x:\{a:Str\}\}$
    3. Убираются подтексты с числом слов меньше $k$
    4. Каждый подтекст вектаризуется с помощью векторизатора $V$
    5. Над порождённым под действием $V$ пространством можно задать метрику $d: \mathbb{R^n} \times \mathbb{R^n} \rightarrow  \mathbb{R}$ (для текстовых эмбедингов себя зарекомендовала косинусное расстояние)
    6. Для каждой вектаризованной подтемы ищем наиболее близкй вектаризованный топик
    7. Убираем все топики, метрика для которых была меньше $\lambda$


    ## Преимущества данного подхода
    
    1. Сохраняется абстрактность алгоритма, из-за чего появляется высокая расширяемость
    2. Легковестность данного подхода, слабая зависимость от *GPU*
    3. Не требует обучение моделей и вызываемых с этим сложностей (разметка данных)

    ## Недостатки

    1. Является эвристикой
    2. Менее точный предобученных моделей
